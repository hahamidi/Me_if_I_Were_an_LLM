{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892af1f4",
   "metadata": {},
   "source": [
    "## Set OpenAI API Key for Teacher–Student Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cbb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95320fef35fd470ba98ce63e489e4e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Password(description='API key:'), Button(description='Set key', style=ButtonStyle()), Output())…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "from ipywidgets import Password, Button, Output, VBox\n",
    "\n",
    "pw = Password(description='API key:')\n",
    "btn = Button(description='Set key')\n",
    "out = Output()\n",
    "\n",
    "def on_click(_):\n",
    "    key = (pw.value or \"\").strip()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = key\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        if key.startswith(\"sk-\"):\n",
    "            print(\"✅ OPENAI_API_KEY set for this session.\")\n",
    "        else:\n",
    "            print(\"⚠️ Set, but it doesn't look like a typical key (no 'sk-').\")\n",
    "\n",
    "btn.on_click(on_click)\n",
    "VBox([pw, btn, out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62b2f6",
   "metadata": {},
   "source": [
    "## Generate Instruction-Tuning Dataset from PDF Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7de4d675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f967a36f1e8e49a983524e6385f046a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "})\n",
      "Saved 25 rows to train.json\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Small dataset maker: PDF ➜ prompt/completion pairs ➜ HF DatasetDict (train)\n",
    "\n",
    "# %% \n",
    "# !pip install -q pypdf datasets openai  # uncomment if needed\n",
    "\n",
    "import os, json, re\n",
    "from typing import List, Dict\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==== CONFIG ====\n",
    "PDF_PATH = \"./Hassan_Hamidi_v825.pdf\"              # <-- put your PDF path here\n",
    "N_EXAMPLES = 25                     # how many examples to generate\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")  # pick a cheap, capable model\n",
    "# You must have OPENAI_API_KEY set in your environment.\n",
    "# e.g., in bash: export OPENAI_API_KEY=\"sk-...\"\n",
    "\n",
    "# (Optional) extra steerage for the kind of instructions you want:\n",
    "SEED_INSTRUCTION = (\n",
    "    \"Current document is a CV, so focus on career, skills, and achievements. use firt person perspective and contextualize the answers as if you are the person in the CV.\"\n",
    "    \"Create diverse, helpful instruction-style Q&A from the document. \"\n",
    "    \"Prefer short, practical prompts and concise, factual answers.\"\n",
    "    \n",
    ")\n",
    "\n",
    "# ==== PDF → TEXT ====\n",
    "def extract_pdf_text(path: str, max_chars: int = 20000) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    chunks = []\n",
    "    total = 0\n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text() or \"\"\n",
    "        if not t:\n",
    "            continue\n",
    "        chunks.append(t)\n",
    "        total += len(t)\n",
    "        if total >= max_chars:\n",
    "            break\n",
    "    return (\"\\n\\n\".join(chunks))[:max_chars]\n",
    "\n",
    "# ==== CALL OPENAI TO GENERATE EXAMPLES ====\n",
    "def gen_examples_from_text(text: str, n: int, model: str) -> List[Dict[str, str]]:\n",
    "    client = OpenAI()\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a data generation assistant for instruction-tuning datasets. \"\n",
    "        \"Use ONLY the provided document text. Avoid fabricating facts.\"\n",
    "    )\n",
    "    user_msg = f\"\"\"\n",
    "Given the document below, create {n} instruction-style prompt/completion pairs.\n",
    "\n",
    "Requirements:\n",
    "- Each item must be an object with keys \"prompt\" and \"completion\".\n",
    "- Prompts should be diverse and useful (requests, how-tos, explanations).\n",
    "- Completions must be concise (<= 120 words), correct, safe, and grounded in the document.\n",
    "- Do NOT include any preamble or commentary.\n",
    "- Return ONLY a JSON array (no markdown fences).\n",
    "\n",
    "Extra steerage: {SEED_INSTRUCTION}\n",
    "\n",
    "DOCUMENT:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "\"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Try to grab the JSON array if the model added anything extra.\n",
    "    m = re.search(r\"\\[\\s*{.*}\\s*\\]\", raw, flags=re.S)\n",
    "    json_str = m.group(0) if m else raw\n",
    "\n",
    "    data = json.loads(json_str)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Model did not return a JSON array.\")\n",
    "    cleaned = []\n",
    "    for ex in data[:n]:\n",
    "        p = (ex.get(\"prompt\") or \"\").strip()\n",
    "        c = (ex.get(\"completion\") or \"\").strip()\n",
    "        if p and c:\n",
    "            cleaned.append({\"prompt\": p, \"completion\": c})\n",
    "    if len(cleaned) != n:\n",
    "        raise ValueError(f\"Expected {n} items, got {len(cleaned)}.\")\n",
    "    return cleaned\n",
    "\n",
    "# ==== RUN ====\n",
    "doc_text = extract_pdf_text(PDF_PATH)\n",
    "examples = gen_examples_from_text(doc_text, N_EXAMPLES, OPENAI_MODEL)\n",
    "\n",
    "train = Dataset.from_list(examples)\n",
    "dset = DatasetDict({\"train\": train})\n",
    "\n",
    "# Save JSONL for training-friendly consumption\n",
    "out_jsonl = \"train.json\"\n",
    "train.to_json(out_jsonl, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(dset)  # Should show: DatasetDict({ train: Dataset({ features: ['prompt','completion'], num_rows: 4 }) })\n",
    "print(f\"Saved {len(train)} rows to {out_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c32ad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8909b04f3e140f7851bee40f9983354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1ef89f6cfe4a49b48ba9ebb65d7865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0992fdfc393c4bbc9693488be7df2f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43f304545fa4e1ca556af4a8f3beef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531912f879094fd4bdd9962b5427cbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e834b383bbc47e689ecea23423874e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4277e8fc9f443239e3032440b4c95e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be467910775346f2b2246b1748e8ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792ee1451b6448c58cd909901fa18ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2cd049585e43568eae2a6bf667ba4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c722a15491664ccc93a1507572b96853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who am I? You are referring to a well-known American singer, songwriter, and actress. You are Mariah Carey. Born on March 27, 1969, in Huntington, New York, she is one of the best-selling music artists of all time, with sales exceeding 200 million units worldwide. She has won numerous awards, including 5 Grammy Awards, and is known for her powerful vocals, distinctive voice, and hit songs like \"All I Want for Christmas Is You,\" \"Hero,\" and \"Emotions.\" Mariah Carey is also a successful actress and entrepreneur. Is there anything specific you'd like to know about her?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "ask_llm = pipeline(\n",
    "    model = model_name,\n",
    "    device =\"cuda\"\n",
    "\n",
    "    \n",
    ")\n",
    "print(ask_llm('who am I?')[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10eeeeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254dc83996204ce3a43ef6fa00851b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "})\n",
      "{'prompt': \"What was your capstone project during your Bachelor's degree?\", 'completion': \"During my Bachelor's degree, my capstone project involved developing a license plates detection system using YOLOv3 and traditional image processing techniques.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_data = load_dataset('json' , data_files='train.json')\n",
    "print(raw_data)\n",
    "x = raw_data['train'][2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9476cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16da4d2e886e40a2980086b811732acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What research are you currently involved in during your PhD?', 'completion': 'I am currently investigating diffusion models for generating high-quality and diverse synthetic medical images. This research supports data augmentation and interpretability, aiming to improve accuracy, fairness, and robustness in disease classification, particularly in rare or imbalanced imaging cases.', 'input_ids': [3838, 3412, 525, 498, 5023, 6398, 304, 2337, 697, 29561, 5267, 40, 1079, 5023, 23890, 57330, 4119, 369, 23163, 1550, 22092, 323, 16807, 27268, 6457, 5335, 13, 1096, 3412, 11554, 821, 78785, 323, 14198, 2897, 11, 37078, 311, 7269, 13403, 11, 50741, 11, 323, 21765, 2090, 304, 8457, 23850, 11, 7945, 304, 8848, 476, 732, 58402, 31658, 5048, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [3838, 3412, 525, 498, 5023, 6398, 304, 2337, 697, 29561, 5267, 40, 1079, 5023, 23890, 57330, 4119, 369, 23163, 1550, 22092, 323, 16807, 27268, 6457, 5335, 13, 1096, 3412, 11554, 821, 78785, 323, 14198, 2897, 11, 37078, 311, 7269, 13403, 11, 50741, 11, 323, 21765, 2090, 304, 8457, 23850, 11, 7945, 304, 8848, 476, 732, 58402, 31658, 5048, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = sample[\"prompt\"] + \"\\n\" + sample['completion']\n",
    "    tokenized = tokenizer(\n",
    "        sample , \n",
    "        max_length= 128,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "\n",
    "data = raw_data.map(preprocess)\n",
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f1e22",
   "metadata": {},
   "source": [
    "## Load Qwen Model and Apply LoRA Configuration for Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb917dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330df8fe2a34e6180532d0a01ccb504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig ,get_peft_model ,TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name , \n",
    "    device_map = \"cuda\" ,\n",
    "    torch_dtype = torch.float16 , \n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    target_modules = [\"q_proj\" , \"k_proj\" , \"v_proj\"]\n",
    "    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model , lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5545474",
   "metadata": {},
   "source": [
    "## Define TrainingArguments and Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99c3c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 01:22, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.615600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=1.3402782917022704, metrics={'train_runtime': 82.567, 'train_samples_per_second': 30.278, 'train_steps_per_second': 4.845, 'total_flos': 5332378583040000.0, 'train_loss': 1.3402782917022704, 'epoch': 100.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments , Trainer \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 100,\n",
    "    learning_rate = 0.0001,\n",
    "    logging_steps = 50,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model ,\n",
    "    args = training_args,\n",
    "    train_dataset = data['train'] \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb560882",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73d78532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./who_am_I_model/tokenizer_config.json',\n",
       " './who_am_I_model/special_tokens_map.json',\n",
       " './who_am_I_model/chat_template.jinja',\n",
       " './who_am_I_model/vocab.json',\n",
       " './who_am_I_model/merges.txt',\n",
       " './who_am_I_model/added_tokens.json',\n",
       " './who_am_I_model/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./who_am_I_model\")\n",
    "tokenizer.save_pretrained(\"./who_am_I_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bd40b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af327adc2a7d481f9b8e59f01a7cffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "MODEL_ADAPTER_PATH = \"./who_am_I_model\"\n",
    "\n",
    "# Load LoRA config and base model\n",
    "peft_cfg = PeftConfig.from_pretrained(MODEL_ADAPTER_PATH)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_cfg.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load adapter weights into base model\n",
    "peft_model = PeftModel.from_pretrained(base_model, MODEL_ADAPTER_PATH)\n",
    "peft_model.eval()\n",
    "\n",
    "# Load tokenizer (uses saved tokenizer in adapter dir)\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(MODEL_ADAPTER_PATH, trust_remote_code=True)\n",
    "if peft_tokenizer.pad_token_id is None:\n",
    "    peft_tokenizer.pad_token = peft_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a12a6",
   "metadata": {},
   "source": [
    "# Ask some question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cd61e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3330e9fc17f44f7cb21a6120bb8f37b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='You are Hassan Hamidi. Answer all questions in my language based only on inform…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Widgets ----------\n",
    "system_box = Textarea(\n",
    "    value=\"You are Hassan Hamidi. Answer all questions in my language based only on information you have. \"\n",
    "          \"Answer short and do not continue the communication after a short answer.\",\n",
    "    description=\"System:\",\n",
    "    layout=dict(width=\"100%\", height=\"80px\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt_box = Textarea(\n",
    "    value=\"What is your current research focus in his PhD program?\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=dict(width=\"100%\", height=\"90px\"),\n",
    ")\n",
    "\n",
    "max_new_tokens = IntSlider(value=128, min=16, max=256, step=16, description=\"Max new\")\n",
    "temperature = FloatSlider(value=0.1, min=0.0, max=2.0, step=0.05, description=\"Temp\")\n",
    "top_p = FloatSlider(value=0.99, min=0.1, max=1.0, step=0.05, description=\"Top-p\")\n",
    "repetition_penalty = FloatSlider(value=1.1, min=1.0, max=2.0, step=0.05, description=\"Rep pen.\")\n",
    "use_chat_template = Checkbox(value=True, description=\"Use chat template (if available)\")\n",
    "\n",
    "generate_btn = Button(description=\"Generate\", button_style=\"primary\")\n",
    "clear_btn = Button(description=\"Clear Output\")\n",
    "status_lbl = Label(value=\"\")\n",
    "\n",
    "out = Output()\n",
    "\n",
    "controls_row1 = HBox([max_new_tokens, temperature, top_p, repetition_penalty])\n",
    "controls_row2 = HBox([use_chat_template, generate_btn, clear_btn, status_lbl])\n",
    "ui = VBox([system_box, prompt_box, controls_row1, controls_row2, out])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "# ---------- Generation handler ----------\n",
    "@torch.inference_mode()\n",
    "def do_generate(_btn):\n",
    "    generate_btn.disabled = True\n",
    "    status_lbl.value = \"Generating...\"\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            system_prompt = system_box.value.strip()\n",
    "            prompt = prompt_box.value.strip()\n",
    "            if not prompt:\n",
    "                print(\"Please enter a prompt.\")\n",
    "                return\n",
    "\n",
    "            if use_chat_template.value and hasattr(peft_tokenizer, \"apply_chat_template\"):\n",
    "                chat = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "                text = peft_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            else:\n",
    "                text = system_prompt + \"\\n\\n\" + prompt\n",
    "\n",
    "            inputs = peft_tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=min(getattr(peft_tokenizer, \"model_max_length\", 4096), 8192),\n",
    "            )\n",
    "\n",
    "            device = next(peft_model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            eos = peft_tokenizer.eos_token_id\n",
    "            gen_ids = peft_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=int(max_new_tokens.value),\n",
    "                do_sample=(temperature.value > 0),\n",
    "                temperature=float(temperature.value),\n",
    "                top_p=float(top_p.value),\n",
    "                repetition_penalty=float(repetition_penalty.value),\n",
    "                eos_token_id=eos,\n",
    "                pad_token_id=peft_tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            text_out = peft_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "            print(text_out)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "        finally:\n",
    "            status_lbl.value = \"Done.\"\n",
    "            generate_btn.disabled = False\n",
    "\n",
    "def do_clear(_btn):\n",
    "    with out:\n",
    "        clear_output()\n",
    "    status_lbl.value = \"\"\n",
    "\n",
    "generate_btn.on_click(do_generate)\n",
    "clear_btn.on_click(do_clear)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
